{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series\n\nIn this kernel I have used a simple set of steps to create an ML model to predict continuous output from 14 continuous features. Some of the key steps include:\n* Feature Engineering\n* Feature Selection using a forward wrapper method\n* Cross validation split\n* Ensemble Model Training and Prediction\n\nSome additional steps that could be implemented to improve this model's accuracy (at the risk of increasing run time) would be:\n* Improve Feature Engineering by making use of more creative methods in tandem with more exploratory data analysis\n* Use a Tree based regressor instead of the linear regressor for feature selection\n* Optimize and tune the hyperparameters of the gradient boosting regressor by using grid search or alternatively use a more complex gradient boosting moel such as LightGBM or XGBoost.","metadata":{}},{"cell_type":"markdown","source":"1. First step will be importing data and storing in a pandas dataframe","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndata = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\nt_data = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\ntest_data = t_data.iloc[:,1:15]\nrow_id = t_data.iloc[:,0]\nX = data.iloc[:,1:15]\nX_copy = X\nY = data.iloc[:,15]\ncolumns = []\nfor col in X.columns:\n    columns.append(col)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-11T11:28:03.608505Z","iopub.execute_input":"2022-11-11T11:28:03.608892Z","iopub.status.idle":"2022-11-11T11:28:04.784275Z","shell.execute_reply.started":"2022-11-11T11:28:03.608842Z","shell.execute_reply":"2022-11-11T11:28:04.783111Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"2. Next some simple feature engineering is done by adding all the columns of the tabular data to one another","metadata":{"execution":{"iopub.status.busy":"2022-11-14T09:04:02.789399Z","iopub.execute_input":"2022-11-14T09:04:02.790263Z","iopub.status.idle":"2022-11-14T09:04:02.813409Z","shell.execute_reply.started":"2022-11-14T09:04:02.790156Z","shell.execute_reply":"2022-11-14T09:04:02.812303Z"}}},{"cell_type":"code","source":"for i in columns:\n    for j in columns[columns.index(i)+1:]:\n        X[i+'_'+j] = X[i]+X[j]\n        test_data[i+'_'+j]= test_data[i]+test_data[j]","metadata":{"execution":{"iopub.status.busy":"2022-11-11T11:28:11.476925Z","iopub.execute_input":"2022-11-11T11:28:11.477325Z","iopub.status.idle":"2022-11-11T11:28:11.764696Z","shell.execute_reply.started":"2022-11-11T11:28:11.477300Z","shell.execute_reply":"2022-11-11T11:28:11.763227Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"3. In order to keep the number of features to a reasonable amount, model specific feature selection is performed, in this case a linear regressor is used to determine the top 20 features by making use of wrapper methods which in turn primarily use p-values to determine the effect a feature has on the target variable ","metadata":{}},{"cell_type":"code","source":"\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LinearRegression\n\nsfs = SFS(LinearRegression(), k_features = 20, forward = True, floating = False, scoring = 'r2', cv =0)\nsfs.fit(X,Y)\ntop_features = sfs.k_feature_names_\nuse_data = X.filter(top_features)\ntest_use_data = test_data.filter(top_features)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T11:13:05.242810Z","iopub.execute_input":"2022-11-11T11:13:05.243156Z","iopub.status.idle":"2022-11-11T11:15:58.215226Z","shell.execute_reply.started":"2022-11-11T11:13:05.243132Z","shell.execute_reply":"2022-11-11T11:15:58.214371Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"4. The data is then prepared to be fed into the model, in order to judge bias and variance of the model we split the training data into a training set and a cross validation set. It is important that the data is binned so we can take a stratified split for the cross validation set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata['target'].describe()\n\nbins = [0,5,6,7,8,9,10,11]\ny_binned = np.digitize(Y,bins)\nX_train,X_val,Y_train,Y_val = train_test_split(use_data,Y, test_size = 0.3, random_state = 42, stratify = y_binned)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T11:17:53.125646Z","iopub.execute_input":"2022-11-11T11:17:53.125985Z","iopub.status.idle":"2022-11-11T11:17:53.297619Z","shell.execute_reply.started":"2022-11-11T11:17:53.125961Z","shell.execute_reply":"2022-11-11T11:17:53.296190Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"5. Finally we make use of an ensemble learning method in the form of a Gradient Boosting Regressor. With the regressor the trick of early stopping is used to determine the number of estimators to use. When the cross validation score does not improve for 3 consecutive estimators the loop is broken and the ideal amount of estimators is determined.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\ngbrt = GradientBoostingRegressor(max_depth = 5, warm_start = True)\nmin_score = np.infty\ncnt = 0\nbuffer = 0\nfor n_estimators in range(1,200):\n    gbrt.n_estimators = n_estimators\n    gbrt.fit(X_train,Y_train)\n    y_pred = gbrt.predict(X_val)\n    score = mean_squared_error(y_pred,Y_val)\n    \n    if score<min_score:\n        min_score = score\n        best_cnt = n_estimators\n        buffer_time = 0\n    else:\n        buffer+=1\n        if buffer>3:\n            break\nprint('best score: ', min_score)\nprint('Optimal Estimators: ', best_cnt)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-11T11:17:56.960749Z","iopub.execute_input":"2022-11-11T11:17:56.961074Z","iopub.status.idle":"2022-11-11T11:24:40.622035Z","shell.execute_reply.started":"2022-11-11T11:17:56.961050Z","shell.execute_reply":"2022-11-11T11:24:40.620685Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"best score:  0.5087072200833646\nOptimal Estimators:  99\n","output_type":"stream"}]},{"cell_type":"markdown","source":"6. Lastly, the data is slightly tweaked so it can be fed into a csv file for submission to the Kaggle Competition","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame(gbrt.predict(test_use_data))\npredictions['id'] = row_id\npredictions.set_axis(['target','id'], axis =1, inplace = True)\npredictions = predictions[['id','target']]\npredictions.to_csv('./submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T11:28:24.075589Z","iopub.execute_input":"2022-11-11T11:28:24.075956Z","iopub.status.idle":"2022-11-11T11:28:24.905011Z","shell.execute_reply.started":"2022-11-11T11:28:24.075925Z","shell.execute_reply":"2022-11-11T11:28:24.903710Z"},"trusted":true},"execution_count":45,"outputs":[]}]}